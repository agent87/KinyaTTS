{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "from typing import Tuple\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "import re\n",
    "\n",
    "import torch\n",
    "\n",
    "from kinyatts.tts.commons import intersperse\n",
    "from kinyatts.tts.utils import get_hparams_from_file, load_checkpoint\n",
    "from kinyatts.tts.models import SynthesizerTrn\n",
    "from kinyatts.tts.text import text_to_sequence\n",
    "from kinyatts.tts.text.symbols import symbols\n",
    "\n",
    "inference_engine = (None, None, None, None)\n",
    "\n",
    "def kinya_tts_setup():\n",
    "    global inference_engine\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "\n",
    "    path_to_tts_config = 'ms_ktjw_istft_vits2_base.json'\n",
    "    path_to_tts_model = 'TTS_MODEL_ms_ktjw_istft_vits2_base_1M.pt'\n",
    "    tts_hps = get_hparams_from_file(path_to_tts_config)\n",
    "    if \"use_mel_posterior_encoder\" in tts_hps.model.keys() and tts_hps.model.use_mel_posterior_encoder == True:\n",
    "        print(\"Using mel posterior encoder for VITS2\")\n",
    "        posterior_channels = 80  # vits2\n",
    "        tts_hps.data.use_mel_posterior_encoder = True\n",
    "    else:\n",
    "        print(\"Using lin posterior encoder for VITS1\")\n",
    "        posterior_channels = tts_hps.data.filter_length // 2 + 1\n",
    "        tts_hps.data.use_mel_posterior_encoder = False\n",
    "    tts_model = SynthesizerTrn(\n",
    "        len(symbols),\n",
    "        posterior_channels,\n",
    "        tts_hps.train.segment_size // tts_hps.data.hop_length,\n",
    "        n_speakers=tts_hps.data.n_speakers, #- >0 for multi speaker\n",
    "        **tts_hps.model).to(device)\n",
    "    _ = tts_model.eval()\n",
    "    _ = load_checkpoint(path_to_tts_model, tts_model, None)\n",
    "\n",
    "    louder_vol = torchaudio.transforms.Vol(gain=3.0, gain_type=\"amplitude\")\n",
    "\n",
    "    inference_engine = (device, tts_model, tts_hps, louder_vol)\n",
    "\n",
    "    print('TTS API engine ready!', flush=True)\n",
    "\n",
    "def get_text(text, hps):\n",
    "    text_norm = text_to_sequence(text)\n",
    "    if hps.data.add_blank:\n",
    "        text_norm = intersperse(text_norm, 0)\n",
    "    text_norm = torch.LongTensor(text_norm)\n",
    "    return text_norm\n",
    "\n",
    "def kinya_tts(inputstr, output_wav_file = 'kinyatts_output.wav') -> Tuple[str,float]: # multi-speaker-tts\n",
    "    global inference_engine\n",
    "    (device, tts_model, tts_hps, louder_vol) = inference_engine\n",
    "    fltstr = re.sub(r\"[\\[\\](){}]\", \"\", inputstr)\n",
    "    stn_tst = get_text(fltstr, tts_hps)\n",
    "    speed = 1\n",
    "    with torch.no_grad():\n",
    "        x_tst = stn_tst.to(device).unsqueeze(0)\n",
    "        x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).to(device)\n",
    "        audio = tts_model.infer(x_tst, x_tst_lengths, noise_scale=.667, noise_scale_w=0.8, length_scale=1 / speed)[0][\n",
    "            0, 0].data.cpu().float()\n",
    "    AUDIO_TIME = audio.size(0) / tts_hps.data.sampling_rate\n",
    "    audio = louder_vol(audio.unsqueeze(0))\n",
    "    torchaudio.save(output_wav_file, audio, tts_hps.data.sampling_rate)\n",
    "    return output_wav_file, AUDIO_TIME\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "kinya_tts_setup()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5c13e859621699b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "inputstr = 'turajijuka buhoro buhoro'\n",
    "\n",
    "wav_file, secs = kinya_tts(inputstr, output_wav_file = 'kinyatts_output.wav')\n",
    "\n",
    "print(f'Synthetized audio file:  {wav_file} ({secs:.1f} seconds)')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6415688d352b2a35"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
